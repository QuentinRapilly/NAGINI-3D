{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"17EwnwKtYfmT3tR8WlDqG7gDOnGCMD9mZ","authorship_tag":"ABX9TyPQtxrO7w1iTU0P2+ex1IH7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Inference on new data\n","\n","This Jupyter Notebook will teach you how to use a previously trained model to segment new images."],"metadata":{"id":"y8KRSmRhckQL"}},{"cell_type":"markdown","source":["## Downloading the package\n","\n","Make sure that the notebook is running with Python>=3.10 and with a version of PyTorch >=1.13 installed (preferably with CUDA available).\n","\n","To verify if PyTorch and Cuda are installed, run the following cell."],"metadata":{"id":"1crk0-b7c3IQ"}},{"cell_type":"code","source":["import torch\n","print(f\"Current version of Pytorch: {torch.__version__}\")\n","print(f\"Cuda working properly: {torch.cuda.is_available()}\")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-rUueKG1d6Lk","executionInfo":{"status":"ok","timestamp":1741166333845,"user_tz":-60,"elapsed":4802,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}},"outputId":"fa3e660f-50eb-4bcd-d418-76631f82cfc9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Current version of Pytorch: 2.5.1+cu124\n","Cuda working properly: True\n"]}]},{"cell_type":"markdown","source":["If you have the good version of Torch and Cuda is working, you can run the following cell to install our package. Otherwise, fix your Python environment before proceeding."],"metadata":{"id":"iuUh9XPzeoDC"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qh3iaG2nahrp","executionInfo":{"status":"ok","timestamp":1741166339503,"user_tz":-60,"elapsed":5656,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}},"outputId":"5a38ccc0-77bf-4bee-9943-1e165d18f507"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nagini3D in /usr/local/lib/python3.11/dist-packages (0.0.1)\n","Requirement already satisfied: csbdeep==0.7.4 in /usr/local/lib/python3.11/dist-packages (from nagini3D) (0.7.4)\n","Requirement already satisfied: einops==0.7.0 in /usr/local/lib/python3.11/dist-packages (from nagini3D) (0.7.0)\n","Requirement already satisfied: omegaconf==2.3.0 in /usr/local/lib/python3.11/dist-packages (from nagini3D) (2.3.0)\n","Requirement already satisfied: scikit-image==0.21.0 in /usr/local/lib/python3.11/dist-packages (from nagini3D) (0.21.0)\n","Requirement already satisfied: scipy==1.11.3 in /usr/local/lib/python3.11/dist-packages (from nagini3D) (1.11.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from csbdeep==0.7.4->nagini3D) (1.26.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from csbdeep==0.7.4->nagini3D) (3.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from csbdeep==0.7.4->nagini3D) (1.17.0)\n","Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from csbdeep==0.7.4->nagini3D) (2025.2.18)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from csbdeep==0.7.4->nagini3D) (4.67.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from csbdeep==0.7.4->nagini3D) (24.2)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf==2.3.0->nagini3D) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf==2.3.0->nagini3D) (6.0.2)\n","Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0->nagini3D) (3.4.2)\n","Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0->nagini3D) (11.1.0)\n","Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0->nagini3D) (2.37.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0->nagini3D) (1.8.0)\n","Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0->nagini3D) (0.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->csbdeep==0.7.4->nagini3D) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->csbdeep==0.7.4->nagini3D) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->csbdeep==0.7.4->nagini3D) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->csbdeep==0.7.4->nagini3D) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->csbdeep==0.7.4->nagini3D) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->csbdeep==0.7.4->nagini3D) (2.8.2)\n"]}],"source":["!pip install nagini3D"]},{"cell_type":"markdown","source":["## Loading the model\n","\n"],"metadata":{"id":"oa-2lvWyfrhl"}},{"cell_type":"code","source":["from nagini3D.models.model import Nagini3D\n","import yaml\n","from os.path import join"],"metadata":{"id":"0YJ92h6Gdrjr","executionInfo":{"status":"ok","timestamp":1741166340783,"user_tz":-60,"elapsed":1275,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Change the paths of following cell to the directories where:\n","- the model is stored (the weigths, the config file and the thresholds file). It can be dowloaded [here](https://zenodo.org/records/14932135?token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjQ5YjY5Mzc0LWRhOWUtNGExZi05YmQ4LTMxOWI1ZWFjYTFiMCIsImRhdGEiOnt9LCJyYW5kb20iOiIzMTNhODA1ZjEzZTYwZDRjNWRhMjMzYzk4MDkxYTIwYyJ9.m8pDDXwVZarpL_sEgtrvMztJgMBaQa_VkusZTIROr-BqkyUI8WNp7MqQI22Si1OfxWNIhp8ei6SCVJFI83iWJg) if you havn't trained your own yet,\n","- the input images are stored. The dataset corresponding to the trained model mentionned before can be downloaded [here](https://zenodo.org/records/14931808?token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjlhYTBlMTRiLTI3YWMtNGIxNi1iNTQxLTcwNjFjMjFlYmE3YiIsImRhdGEiOnt9LCJyYW5kb20iOiI2MDdmMmQ3NzdjZWMyNDM1NTA4ZjI4OTUzYmQ3OWU3MiJ9.rJK8i6DmDl75V3fxJNIm63LeXsm0uHrOGoOc4mtiYOxBLGSAzfzfu04QlZft5eKr38c-r8exYpDE_ZqqBURldg),\n","- you want to store the results."],"metadata":{"id":"fAMvEPDLgi5Q"}},{"cell_type":"code","source":["model_dir = \"/content/drive/MyDrive/INRIA/Serpico/Test_nagini/model_save\"\n","input_path = \"/content/drive/MyDrive/INRIA/Serpico/Test_nagini/image\"\n","result_path = \"/content/drive/MyDrive/INRIA/Serpico/Test_nagini/results\""],"metadata":{"id":"0zZ-Qmo1gnv6","executionInfo":{"status":"ok","timestamp":1741166340785,"user_tz":-60,"elapsed":2,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["weigth_file = \"best.pkl\"\n","\n","with open(join(model_dir, \"config.yaml\"), 'r') as file:\n","  cfg = yaml.safe_load(file)\n","\n","with open(join(model_dir, \"thresholds.yaml\"), 'r') as file:\n","  thresholds = yaml.safe_load(file)\n","\n","M1 = cfg[\"settings\"][\"M1\"]\n","M2 = cfg[\"settings\"][\"M2\"]\n","r_mean = cfg[\"settings\"][\"r_mean\"]\n","\n","weight_path = join(model_dir, weigth_file)\n","\n","model = Nagini3D(unet_cfg=cfg[\"model\"], P = 101, M1 = M1, M2 = M2, save_path=result_path, device=device)\n","model.load_weights(weight_path)\n","\n","proba_th = thresholds[\"prob\"]\n","nms_th = thresholds[\"nms\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qItI-7BnfdXT","executionInfo":{"status":"ok","timestamp":1741166341084,"user_tz":-60,"elapsed":293,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}},"outputId":"5e7b96d1-5198-49a3-b933-043b5c298ef3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Num layers : 3 Features Start : 32 Padding Mode : zeros\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/nagini3D/models/model.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.model.load_state_dict(torch.load(weights_file, map_location=self.device))\n"]}]},{"cell_type":"markdown","source":["## Applying the loaded model"],"metadata":{"id":"lZy6xLvpzaYh"}},{"cell_type":"markdown","source":["The parameters that you can tune for the inference are:\n","- if `snake_optim = True` the snake refinement process will be applied after the network prediction. If False, the network output is provided directly.\n","- if `use_ostu = True` the snake refinement step will be applied using a binarized version of the input image (computed using Otsu thresholding). That is particularly efficient on images with sparse objects. If objects are dense, set this parameter to False."],"metadata":{"id":"kszS5awvkCg5"}},{"cell_type":"code","source":["snake_optim = True\n","use_otsu = True"],"metadata":{"id":"0mqTxqm6kDTB","executionInfo":{"status":"ok","timestamp":1741166341092,"user_tz":-60,"elapsed":8,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["If you have issues with the memory (\"CUDA out of memory\") when you try to process images, consider spliting your images in tiles using the following cell.\n","Indicate the number of splits along X,Y,Z axis."],"metadata":{"id":"Y6idGdmOiAQy"}},{"cell_type":"code","source":["nb_tiles = 1,1,1"],"metadata":{"id":"Nf0JCL13h9eL","executionInfo":{"status":"ok","timestamp":1741166341094,"user_tz":-60,"elapsed":1,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import tifffile\n","from glob import glob\n","from os.path import basename\n","\n","imgs_path = glob(join(input_path, \"*.tif\"))\n","\n","for img_f in imgs_path:\n","  img = tifffile.imread(img_f)\n","\n","  with torch.no_grad():\n","    mask, proba, points = model.inference(img, proba_th=proba_th, r_mean=r_mean, nb_tiles=nb_tiles, nms_th=nms_th, optim_snake=snake_optim)\n","\n","  tifffile.imwrite(join(result_path, basename(img_f)), mask.cpu().numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILiwr9B8iO_f","executionInfo":{"status":"ok","timestamp":1741166375365,"user_tz":-60,"elapsed":34271,"user":{"displayName":"Quentin RAPILLY","userId":"13440189952482936627"}},"outputId":"e1b7e2c9-c559-47ee-df46-17431ba8a0dd"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Infering tile : X : [0, 200], Y : [0, 200], Z : [0, 200]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/nagini3D/models/tools/inference_tools.py:123: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n","Please either pass the dim explicitly or simply use torch.linalg.cross.\n","The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)\n","  n = torch.cross(p1,p2)                                         # same\n"]}]}]}